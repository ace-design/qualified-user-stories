# Ground Truth Definition Protocol

To build the ground truth, we manually annotated Dalpiaz et al.'s dataset, using the [Doccano](https://doccano.herokuapp.com/) platform, as a _Named Entity Recognition_ task. We loaded the different labels (_Persona_, _Action_, _Entity_, _Benefit_) and two relations (_triggers_, _contains). The annotation process was the responsibility of the 1st author. 

To ensure the quality of the annotated set, we introduced several cross-checks: _(i)_ an initial calibration was done in collaboration with the 3rd author on _75_ stories randomly sampled, _(ii)_ validation meetings were held on a bi-weekly basis over two months during the annotation phase, and _(iii)_ the 3rd author cross-checked manually _330_ randomly sampled annotated stories (19.6%).
No significant deviations were found during this step, as only _21_ stories containing (minor) _disputable_ elements were identified, leading to an inter-rater agreement of 94\% for this sample, which is classically considered as excellent.

In case of significant disputes, the 2nd author would have acted as an external referee and, as such, was kept out of the annotation process 

Overall, the annotation phase lengthed two months, and consumed in total 10 days for the 1st author, and 1.5 days for the 3rd author (not counting the bi-weekly meetings).

